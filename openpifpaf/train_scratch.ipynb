{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import openpifpaf\n",
    "\n",
    "openpifpaf.show.Canvas.show = True\n",
    "openpifpaf.show.Canvas.image_min_dpi = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenPifPaf version 0.12.13\n",
      "PyTorch version 1.9.1+cu102\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import torch\n",
    "import cv2\n",
    "import copy\n",
    "import time\n",
    "\n",
    "from openpifpaf import datasets, encoder, logger, network, optimize, plugin, show, visualizer,train\n",
    "\n",
    "from openpifpaf.network import trainer\n",
    "\n",
    "print('OpenPifPaf version', openpifpaf.__version__)\n",
    "print('PyTorch version', torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.argv = ['']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-d87f8aad572d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcli\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdatamodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfactory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mnet_cpu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFactory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfactory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead_metas\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdatamodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead_metas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/openpifpaf/train.py\u001b[0m in \u001b[0;36mcli\u001b[0;34m()\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0moptimize\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcli\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparser\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcli\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparser\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m     \u001b[0mshow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcli\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparser\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m     \u001b[0mvisualizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcli\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparser\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/openpifpaf/show/cli.py\u001b[0m in \u001b[0;36mcli\u001b[0;34m(parser)\u001b[0m\n\u001b[1;32m     12\u001b[0m     group.add_argument('--save-all', nargs='?', default=None, const='all-images/',\n\u001b[1;32m     13\u001b[0m                        help='every plot is saved (optional to specify directory)')\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mCanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mAnimationFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     group.add_argument('--show', default=False, action='store_true',\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "args = train.cli()\n",
    "\n",
    "datamodule = datasets.factory(args.dataset)\n",
    "\n",
    "net_cpu, start_epoch = network.Factory().factory(head_metas=datamodule.head_metas)\n",
    "loss = network.losses.Factory().factory(net_cpu.head_nets)\n",
    "\n",
    "checkpoint_shell = None\n",
    "if not args.disable_cuda and torch.cuda.device_count() > 1 and not args.ddp:\n",
    "    checkpoint_shell = copy.deepcopy(net_cpu)\n",
    "    net = torch.nn.DataParallel(net_cpu.to(device=args.device))\n",
    "    loss = loss.to(device=args.device)\n",
    "elif not args.disable_cuda and torch.cuda.device_count() == 1 and not args.ddp:\n",
    "    checkpoint_shell = copy.deepcopy(net_cpu)\n",
    "    net = net_cpu.to(device=args.device)\n",
    "    loss = loss.to(device=args.device)\n",
    "elif not args.disable_cuda and torch.cuda.device_count() > 0:\n",
    "    assert not list(loss.parameters())\n",
    "    assert torch.cuda.device_count() > 0\n",
    "    checkpoint_shell = copy.deepcopy(net_cpu)\n",
    "    torch.cuda.set_device(args.local_rank)\n",
    "    torch.distributed.init_process_group(backend='nccl', init_method='env://')\n",
    "    if args.sync_batchnorm:\n",
    "        net_cpu = torch.nn.SyncBatchNorm.convert_sync_batchnorm(net_cpu)\n",
    "    else:\n",
    "        net = torch.nn.parallel.DistributedDataParallel(\n",
    "            net_cpu.to(device=args.device),\n",
    "            device_ids=[args.local_rank], output_device=args.local_rank,\n",
    "            find_unused_parameters=isinstance(datamodule, datasets.MultiDataModule),\n",
    "    )\n",
    "    loss = loss.to(device=args.device)\n",
    "else:\n",
    "    net = net_cpu\n",
    "\n",
    "# logger.train_configure(args)\n",
    "train_loader = datamodule.train_loader()\n",
    "val_loader = datamodule.val_loader()\n",
    "if torch.distributed.is_initialized():\n",
    "    train_loader = datamodule.distributed_sampler(train_loader)\n",
    "    val_loader = datamodule.distributed_sampler(val_loader)\n",
    "\n",
    "optimizer = optimize.factory_optimizer(\n",
    "    args, list(net.parameters()) + list(loss.parameters()))\n",
    "lr_scheduler = optimize.factory_lrscheduler(\n",
    "    args, optimizer, len(train_loader), last_epoch=start_epoch)\n",
    "trainer = network.Trainer(\n",
    "    net, loss, optimizer, args.output,\n",
    "    checkpoint_shell=checkpoint_shell,\n",
    "    lr_scheduler=lr_scheduler,\n",
    "    device=args.device,\n",
    "    model_meta_data={\n",
    "        'args': vars(args),\n",
    "        'plugin_versions': plugin.versions()\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.loop(train_loader, val_loader, start_epoch=start_epoch)\n",
    "\n",
    "# trainer.train(train_loader, start_epoch)\n",
    "\n",
    "start_time = time.time()\n",
    "trainer.model.train()\n",
    "if trainer.fix_batch_norm is True \\\n",
    "    or (trainer.fix_batch_norm is not False and trainer.fix_batch_norm <= start_epoch):\n",
    "    for m in trainer.model.modules():\n",
    "        if isinstance(m, (torch.nn.BatchNorm1d, torch.nn.BatchNorm2d)):\n",
    "            m.eval()\n",
    "\n",
    "trainer.ema_restore()\n",
    "trainer.ema = None\n",
    "\n",
    "epoch_loss = 0.0\n",
    "head_epoch_losses = None\n",
    "head_epoch_counts = None\n",
    "last_batch_end = time.time()\n",
    "trainer.optimizer.zero_grad()\n",
    "\n",
    "for batch_idx, (data, target, _) in enumerate(train_loader):\n",
    "    print(data.shape)\n",
    "    preprocess_time = time.time() - last_batch_end\n",
    "\n",
    "    # Train the batches on the pif paf \n",
    "    batch_start = time.time()\n",
    "    apply_gradients = batch_idx % trainer.stride_apply == 0\n",
    "    loss, head_losses = trainer.train_batch(data, target, apply_gradients)\n",
    "\n",
    "    # update epoch accumulates\n",
    "    if loss is not None:\n",
    "        epoch_loss += loss\n",
    "    if head_epoch_losses is None:\n",
    "        head_epoch_losses = [0.0 for _ in head_losses]\n",
    "        head_epoch_counts = [0 for _ in head_losses]\n",
    "    for i, head_loss in enumerate(head_losses):\n",
    "        if head_loss is None:\n",
    "            continue\n",
    "        head_epoch_losses[i] += head_loss\n",
    "        head_epoch_counts[i] += 1\n",
    "\n",
    "    batch_time = time.time() - batch_start\n",
    "\n",
    "    if batch_idx % trainer.log_interval == 0:\n",
    "        batch_info = {\n",
    "            'type': 'train',\n",
    "            'epoch': start_epoch, 'batch': batch_idx, 'n_batches': len(train_loader),\n",
    "            'time': round(batch_time, 3),\n",
    "            'data_time': round(preprocess_time, 3),\n",
    "            'lr': round(trainer.lr(), 8),\n",
    "            'loss': round(loss, 3) if loss is not None else None,\n",
    "            'head_losses': [round(l, 3) if l is not None else None\n",
    "                            for l in head_losses],\n",
    "        }\n",
    "\n",
    "        if hasattr(trainer.loss, 'batch_meta'):\n",
    "            batch_info.update(trainer.loss.batch_meta())\n",
    "\n",
    "    # initialize ema\n",
    "    if trainer.ema is None and trainer.ema_decay:\n",
    "        trainer.ema = copy.deepcopy([p.data for p in trainer.model.parameters()])\n",
    "\n",
    "    # update learning rate\n",
    "    if trainer.lr_scheduler is not None:\n",
    "        trainer.lr_scheduler.step()\n",
    "\n",
    "    if trainer.n_train_batches and batch_idx + 1 >= trainer.n_train_batches:\n",
    "        break\n",
    "\n",
    "    last_batch_end = time.time()\n",
    "\n",
    "    break\n",
    "\n",
    "trainer.apply_ema()\n",
    "trainer.n_clipped_grad = 0\n",
    "trainer.max_norm = 0.0\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "42f01ea88de992bfe030ae62812b3bd75e246cedf5d649171f226dc078740c9d"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('pifpaf': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
